---
title: "Virginia Trout Water Locations"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
library(stringdist)
library(osmdata)
library(ggplot2)

library(kableExtra)

library(textreuse)

source("../R/utils.R")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
va <- sf::st_read("../data/gis/cb_2019_us_state_20m/cb_2019_us_state_20m.shp", quiet=TRUE) %>% 
  filter(NAME=="Virginia") %>%
  sf::st_transform(4326)

vaco1 <- sf::st_read("../data/gis/cb_2019_us_county_20m/cb_2019_us_county_20m.shp", quiet=TRUE) %>%
  filter(STATEFP == "51") %>%
  sf::st_transform(4326)

# combine City/County with same name, to avoid confusion
vaco2 <- vaco1 %>% group_by(NAME) %>% summarise(geometry = sf::st_union(geometry))

df.trout <- readRDS("../data/rds/dftrout.rds")

df.trt <- df.trout %>% 
  dplyr::select(waterbody1, waterbody2, Category, specialtype, county) %>%
  distinct() %>%
  mutate(id = sprintf("trt-%03d", row_number()))    #paste0("trt-",row_number()))
```


<hr>
<br>


Water bodies that are included in the Virginia Department of Wildlife Recreation (VDWR) trout stocking program can be browsed on an [interactive map](https://dgif-virginia.maps.arcgis.com/apps/webappviewer/index.html?id=441ed456c8664166bb735b1db6024e48), however there doesn't appear to be a simple way for the average user to download the location data.  For this reason, we explore some methods for determining these locations in the sections below.

<br>

## Brute Force Method

The no-tech solution to this problem is to simply brute-force our way through the trout water locations by manually searching for each individual location.  It's frustrating to know that the locations are definitely recorded in a database somewhere, just not easily accessible to me personally, so I literally pasted each "trout water" into the search bar on the interactive map, navigated to the appropriate search result, dropped a pin on the location, and copied the relevant coordinates.  

As painful as this was, it was manageable (it took about 3-ish hours), and very illuminating.  For example, I found a handful of inconsistencies like misspellings, alternative names, or missing waterways.  In some cases I resorted to a manual google-maps search to help narrow the field and made a judgement call as to the probable location of some waterway.  In all cases I entered a discrete point, even though many of the locations are rivers and streams and trout stocking is probably carried out at a variety of locations along the waterway.  In one case I wasn't able to find the location at all because the name, "Forest Service Office Youth Ponds" was very much ambiguous and I couldn't find where the Forest Service Office is located (apparently even Google doesn't know!); in that case I used the center of the respective county. 

<br>

## VDEQ Data

```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# First, download these files from:
# https://geohub-vadeq.hub.arcgis.com/pages/Water%20Datasets
# Then read them with sf
df.rvr <- sf::st_read("../data/gis/Rivers_(2020_Final_WQA_IR_Assessment)/Rivers_(2020_Final_WQA_IR_Assessment).shp", quiet=TRUE)
df.res <- sf::st_read("../data/gis/Reservoirs_(2020_Final_WQA_IR_Assessment)/Reservoirs_(2020_Final_WQA_IR_Assessment).shp", quiet=TRUE)


epsg_code <- spaceheater::get_epsg(df.rvr$geometry)
EPSG_WGS84 <- spaceheater::epsg_wgs84()

find_county <- function(crd) {
  vv <- sf::st_intersects(crd, vaco1$geometry) %>% unlist()
  nm <- ifelse(length(vv) > 0, vaco1$NAME[vv], NA)
  return(nm)
}


# Combine Rivers & Streams with Lakes & Reservoirs
df.deq <- bind_rows(
  # Rivers & Streams
  df.rvr %>%
  mutate(coord = geometry %>% sf::st_transform(epsg_code) %>% sf::st_centroid() %>% sf::st_transform(EPSG_WGS84),
         source = "rvr") %>%
  as_tibble() %>%
  dplyr::select(ID305B, WATER_NAME, LOCATION, source, coord),
  # Lakes & Reservoirs
  df.res %>% 
      mutate(coord = geometry %>% sf::st_transform(epsg_code) %>% sf::st_centroid() %>% sf::st_transform(EPSG_WGS84),
             source = "res") %>%
      as_tibble() %>%
      dplyr::select(ID305B, WATER_NAME, LOCATION, source, coord)
) %>%
  mutate(county = purrr::map_chr(coord, find_county))


# Note: some of the coordinates are outside of the state because
# of how the crds are derived above.  Note also that this could, in 
# some rare cases, mean that the county is also incorrect?
df.sub <- df.deq %>% 
  filter(is.na(county)) %>%
  mutate(coord2 = coord %>% 
           sf::st_transform(epsg_code) %>% 
           sf::st_buffer(dist=5000) %>% 
           sf::st_transform(EPSG_WGS84),
         county2 = purrr::map_chr(coord2,find_county))


df.deqx <- df.deq %>% 
  left_join(df.sub %>% dplyr::select(ID305B, county2), by=c("ID305B")) %>%
  mutate(county = ifelse(is.na(county), county2, county)) %>%
  dplyr::select(-county2)


# Save files
saveRDS(df.deqx, "../data/rds/df_deq.rds")
readr::write_csv(df.deqx, "data/csv/df_deq.csv")
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
df.rvr <- sf::st_read("../data/gis/Rivers_(2020_Final_WQA_IR_Assessment)/Rivers_(2020_Final_WQA_IR_Assessment).shp", quiet=TRUE)
df.res <- sf::st_read("../data/gis/Reservoirs_(2020_Final_WQA_IR_Assessment)/Reservoirs_(2020_Final_WQA_IR_Assessment).shp", quiet=TRUE)

df.deq <- readRDS("../data/rds/df_deq.rds")
df.deqt <- readRDS("../data/rds/df_deq_t.rds")
```



The Virginia Department of Environmental Quality (VDEQ) maintains an up-to-date [collection of datasets](https://geohub-vadeq.hub.arcgis.com/pages/Water%20Datasets) related to rivers/streams, and lakes/reservoirs, plus a variety of other features.  Combining the rivers/streams and lakes/reservoirs data yields `r unique(df.deq$ID305B) %>% length() %>% format(big.mark=",")` unique bodies of water, or `r unique(df.deq$WATER_NAME) %>% length() %>% format(big.mark=",")` uniquely named bodies of water.  What's the difference?  Many of the bodies of water are divided into multiple segments and some are frequently-used names.  For example, the James River has `r df.deq %>% filter(WATER_NAME=="James River") %>% nrow()` sections outlined in the VDEQ data, and Mill Creek has `r df.deq %>% filter(WATER_NAME=="James River") %>% nrow()`.  If we look at the geographic distribution of the respective points, the difference becomes clear,

```{r, echo=FALSE, out.width="60%", fig.align="center"}
df.deq %>% 
  filter(WATER_NAME %in% c("James River", "Mill Creek")) %>% 
  ggplot() + 
  geom_sf(data=vaco1, mapping=aes(geometry=geometry), size=0.2, fill="grey95", color="grey65") +
  geom_sf(aes(geometry=coord, color=WATER_NAME)) + 
  scale_color_discrete(name="Water Name") +
  theme_void() +
  theme(legend.position = c(0.2, 0.8))


```

To a large degree these multiplicity issues can be resolved by filtering the data by county (whereas counties aren't explicitly listed in the VDEQ data, we derive these from shapes available from the [Census Bureau](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html)).  Rivers, however, may require an extra step towards disambiguation.  Nevertheless, it's worth noting that around 75% of the names have only a single entry, so these problems shouldn't be too pervasive.

Note: after carrying out 90% of this analysis, I discovered that VDEQ has two additional datasets that are specifically labeled as Trout Rivers & Streams and Trout Lakes & Reservoirs.  We will revisit these datasets later. 


### Pairwise comparisons

Comparing water bodies between these two datasets is nontrivial.  As noted in the section above describing the brute-force approach, there are several cases where misspellings, alternative spellings, ambiguous names, or missing data come into play.  Add to this the multiplicities highlighted above for the VDEQ data.  With all of this in mind, the strategy here will be to formulate a set of logical rules that will help us come up with a "best guess" as to which trout waters denoted in the VDWR data correspond to which water bodies in the VDEQ data.  The initial procedure will look like this,

<ol>
  <li>Filter by county</li>
  <li>Determine name similarity</li>
  <li>Filter by jaccard distance</li>
</ol>

<details>
<summary style="font-size:15px">CODE</summary>
```{r, message=FALSE, warning=FALSE}
#
# Pairwise comparisons
#
t0 <- Sys.time()


d <- lapply(1:nrow(df.trt), function(i) {
  df.deq %>% 
    filter(county == df.trt$county[i]) %>%
    mutate(waterbody1 = df.trt$waterbody1[i],
           jc = stringdist::stringdist(waterbody1, WATER_NAME, method="jaccard",q=3),
           id = df.trt$id[i]) %>%
    filter(jc<0.50) %>%
    arrange(id, jc) %>%
    dplyr::select(id, waterbody1, ID305B, WATER_NAME, county, jc, source, coord)
})


df.cnd1 <- do.call(rbind, d)
t1 <- Sys.time()
DT_PR <- t1-t0
```
</details>
<br>

The result is a list of candidate pairs that will be used later in the <b>Adjudication</b> step.  In the table below, a Jaccard score of zero means the names are identical.  Some things worth noting here, which will be discussed in greater depth later, 1) some cases return a single result with exact string match, 2) some cases return multiple results with exact string matches, 3) some cases return a mixture of exact and inexact string matches 4) some cases return only inexact string matches, and 5) some trout waters do not return any candidate pairs.  The adjudication step will need to account for each of these scenarios.  Some of these scenarios are evident in the sample printed below,

```{r, echo=FALSE}
df.cnd1 %>%
  dplyr::select(id, waterbody1, WATER_NAME, jc) %>%
  head(n=10) %>%
  kbl(col.names=c("id","Trout Water", "VDEQ water", "Jaccard"), digits=2) %>%
  kable_styling(full_width=FALSE) %>%
  kable_paper()
```

<br>

### Locality-sensitive hashing

Locality-sensitive hashing is a technique that can be used to reduce the search space in cases where pairwise comparisons between datasets may become intractable.  The methodology, described [here](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf), takes an approximative approach that mimics the jaccard similarity.  The data we are working with here don't quite meet the threshold of intractability, however it is interesting to go through the mechanics of LSH anyway.

It's worth noting, however, that LSH is typically discussed within the context of finding similar/duplicate items within a large document corpus, whereas the problem being addressed here is specifically related to the comparison of two different datasets (this may or may not be strictly true; it's possible that this is a limitation of the specific implementation used here, or it might be related to my own lack of familiarity with the implementation).  

The overarching goal here is to use LSH to create a list of candidate pairs.  Most of the necessary steps are carried out with the `textreuse` package, however since the implementation appears to be designed for use with longer documents, we will carry out some pre-processing steps to tokenize text by characters instead of the default, which is by word.

<details>
<summary style="font-size:15px">CODE</summary>
```{r}
# Pre-processing: VDEQ data
df.deq2 <- df.deq %>%
  mutate(ngrams     = tokenizers::tokenize_character_shingles(WATER_NAME, n=3,   # tokenize by ngram
                                                              strip_non_alphanum=FALSE,
                                                              simplify=TRUE),  
         len        = purrr::map_int(ngrams, length),                            # number of tokens
         txt_tokens = purrr::map_chr(ngrams, str_c,collapse=" "))                # prep for minhash


# Pre-processing: VDWR data
df.trt2 <- df.trt %>%
  mutate(ngrams     = tokenizers::tokenize_character_shingles(waterbody1, n=3,
                                                          strip_non_alphanum=FALSE,
                                                          simplify=TRUE),
         len        = purrr::map_int(ngrams, length),
         txt_tokens = purrr::map_chr(ngrams, str_c, collapse=" "))
```
</details>
<br>

In the present case the water body names are tokenized into ngrams, where n=3.  It is useful to take a look at the number of ngrams per name for each dataset,  

```{r, out.width="50%", echo=FALSE, message=FALSE, warning=FALSE, fig.pos="hold"}
df.trt2 %>% 
  ggplot() +
  geom_histogram(aes(x=len), bins=25) +
  lims(x=c(0,50)) +
  labs(title="VDWR Trout Waters", x = "Number of ngrams per row", y="Count") +
  theme_minimal()

df.deq2 %>% 
  ggplot() +
  geom_histogram(aes(x=len), bins=25) +
  lims(x=c(0,50)) +
  labs(title="VDEQ Water Bodies", x = "Number of ngrams per row", y="Count") +
  theme_minimal()
```
The histograms show an interesting tri-modal feature, most evident in the VDEQ data.  Closer inspection reveals that most of the names in the VDEQ data with greater than 30 ngrams follow a pattern like "Unnamed tributary to...".  The peak around 20 mostly consists of more complex names in both datasets, such as those specifying "North Fork.." or "South Fork...", or something like "Goose Creek/Crooked Run/Gap Run".

The minhashing workflow here is based largely on the `textreuse` package [vignette](https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html).  We start by generating a minhash function and using that to create a document corpus containing all of the minhashed ngrams.  The minhash function converts a collection of tokens into `n` randomly selected hashes.  In the present dataset, there are `r df.trt2 %>% pull(ngrams) %>% unlist() %>% unique() %>% length() %>% format(big.mark=",")` unique ngrams derived from the trout waters, and `r df.deq2 %>% pull(ngrams) %>% unlist() %>% unique() %>% length() %>% format(big.mark=",")` from the VDEQ water body names.  These are mapped to 240 hashes.

<details>
<summary style="font-size:15px">CODE</summary>
```{r}
# Create minhash function
minhash <- textreuse::minhash_generator(n=240, seed=1234)
```
</details>
<br>

The document corpus is created, which consists of all names from both datasets.

<details>
<summary style="font-size:15px">CODE</summary>
```{r}
# Create a named vector of tokens for each row
vec_tokens <- c(df.deq2$txt_tokens, df.trt2$txt_tokens)
names(vec_tokens) <- c(df.deq2$ID305B, df.trt2$id)

# Generate corpus
t0 <- Sys.time()
corpus <- textreuse::TextReuseCorpus(text = vec_tokens, tokenizer = tokenizers::tokenize_ngrams, n = 3,
                          minhash_func = minhash, keep_tokens = TRUE,
                          progress = FALSE, simplify=TRUE)
t1 <- Sys.time()
DT_LS1 <- t1-t0
```
</details>
<br>

Locality-sensitive hashing can now be carried out on the corpus.  The only remaining parameter is the number of bands to use in the computation.  The approximate threshold for jaccard similarity used to create the candidate pairs depends on this value.  In the present case $b=40$ is used since it leads to an approximate threshold of around 0.5.  


```{r, echo=FALSE, out.width="25%", eval=FALSE}
v <- c(10, 15, 30, 40, 60, 80, 120, 240)
d <- lapply(1:length(v), function(i) {
  lsh_threshold(h=240, b=v[i])
})
data.frame(b = v, similarity = unlist(d) %>% round(digits=3)) %>%
  kableExtra::kbl() %>%
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(full_width=FALSE)
```

<details>
<summary style="font-size:15px">CODE</summary>
```{r}
t0 <- Sys.time()
buckets_040    <- lsh(corpus, bands = 40, progress = FALSE)
candidates_040 <- lsh_candidates(buckets_040) %>% mutate(bands=40)
t1 <- Sys.time()
DT_LS2 <- t1-t0
```
</details>
<br>

The result is a list of candidate pairs, nominally, pairs which will yield a jaccard similarity less than the theshold value implied by the choice of $b$.  With a little bit of post-processing one can generate a result analogous to what was produced above,

<details>
<summary style="font-size:15px">CODE</summary>
```{r}
t0 <- Sys.time()
df.cnd2 <- candidates_040 %>%
  filter(a %in% df.trt2$id) %>%
  rename(id=a, ID305B=b) %>%
  left_join(df.trt %>% dplyr::select(id, waterbody1, county), by=c("id")) %>%
  left_join(df.deq %>% dplyr::select(ID305B, WATER_NAME, source, coord, county2=county), by=c("ID305B")) %>%
  filter(county==county2) %>%
  mutate(jc = stringdist::stringdist(waterbody1, WATER_NAME, method="jaccard",q=3)) %>%
  arrange(id, jc) %>%
  dplyr::select(id, waterbody1, ID305B, WATER_NAME, county, jc, source, coord)
t1 <- Sys.time()
DT_LS3 <- t1-t0
DT_LS <- DT_LS1 + DT_LS2 + DT_LS3
```
</details>
<br>


```{r, echo=FALSE, eval=FALSE}
# One can also use textreuse to compute the similarity.
# I used the above method so I can compare with the pairwise method
df.compare2 <- candidates_040 %>% filter(a %in% df.trt2$id) %>%
  lsh_compare(corpus, jaccard_similarity, progress=FALSE) %>%
  arrange(a,desc(score))
```

```{r, echo=FALSE}
df.cnd2 %>%
  dplyr::select(id, waterbody1, WATER_NAME, jc) %>%
  head(n=10) %>%
  kbl(col.names=c("id","Trout Water", "VDEQ water", "Jaccard"), digits=2) %>%
  kable_styling(full_width=FALSE) %>%
  kable_paper()
```



### Comparison of Pairwise and LSH methods

In principle, the _Pairwise_ and _LSH_ methods outlined above do more or less the same thing, so it is interesting to do a quick comparison of the results.

First, ignoring some of the pre-processing steps, the Pairwise method took about <b>`r DT_PR %>% as.numeric() %>% signif(digits=1)` s</b> to run, whereas the LSH method took around <b>`r DT_LS %>% as.numeric() %>% signif(digits=1)` s</b> to generate the corpus (`r DT_LS1 %>% as.numeric() %>% format(digits=1)`s), compute locality-sensitive hashes (`r DT_LS2 %>% as.numeric() %>% format(digits=1)`s), and carry out some light post-processing (`r DT_LS3 %>% as.numeric() %>% format(digits=1)`s).  Note that this not a completely fair comparison, and there may be more optimization to be carried out.  For example, the pairwise method doesn't compare all records with all other records; instead it compares each record from the trout data to all records in the VDEQ data _that are in the same county_.  In effect, we are carrying out a blocking process prior to the comparing the datasets.  In the LSH method detailed above this blocking step is carried out as part of the post-processing.  Not only that, but the corpus doesn't distinguish between datasets, so the LSH method is intrinsically comparing each record with all other records _in both datasets_.  

Next, it is worth comparing the output of each method,

```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width="48%", fig.show="hold"}
p1 <- df.cnd1 %>% 
  ggplot() + 
  geom_histogram(aes(x=jc),bins=50) + 
  lims(x=c(-0.05,1), y=c(0,500)) +
  labs(title="Candidate Pairs: Pairwise Method",
       subtitle=paste("Total Candidate Pairs:",nrow(df.cnd1)),
       x="Jaccard Similarity", y="Count") +
  theme_minimal()

p2 <- df.cnd2 %>% 
  ggplot() + 
  geom_histogram(aes(x=jc),bins=50) + 
  lims(x=c(-0.05,1), y=c(0,500)) +
  labs(title="Candidate Pairs: Locality-Sensitive Hashing",
       subtitle=paste("Total Candidate Pairs:",nrow(df.cnd2)),
       x="Jaccard Similarity", y="Count") +
  theme_minimal()

p1
p2
```

Notice that the Jaccard similarity shows a sharp cutoff at 0.5 for the Pairwise method, but the boundary is fuzzier with LSH.  This is a consequence of the latter being an approximate method.  It may be possible to refine the LSH parameters in a way so as to create a sharper cutoff. Also, the distribution of similarity scores is different for the two methods as the value becomes larger, however the number of exact matches is identical.  This has to do with the probabilistic nature of LSH. 

<br>

### Adjudication

The candidate pairs identified above ideally represent the same waterways present in both datasets, however the reality is more complicated and nuanced.  For example, as was briefly mentioned above, there are five possible outcomes for each candidate pair,

<ol>
  <li>Single exact match</li>
  <li>Multiple exact matches</li>
  <li>Mixture of exact and inexact matches</li>
  <li>Inexact matches</li>
  <li>No candidates</li>
</ol>

During the adjudication process the goal will be to assign a likelihood value to each proposed candidate pair.  To do this, we will use Bayes Law,

$$
Pr(\theta | y) = \frac{Pr(y | \theta) Pr(\theta)}{Pr(y)}
$$
where,

<ul>
  <li>$Pr(\theta | y)$ is the probability that the candidate pair is correct, given similarity score $y$ </li>
  <li>$Pr(y | \theta)$ is the probability of similarity score $y$, given the candidate pair is correct </li>
  <li>$Pr(\theta)$ is the prior probability, or the probability that the candidate pair is correct </li>
  <li>$Pr(y)$ is the probability of similarity score $y$</li>
</ul>

The value for $Pr(y | \theta)$ would nominally be determined from data.  For example, if a subset of the candidate pairs were adjudicated manually or otherwise known _a priori_, this information could be used to derive an expression for the probability of similarity score $y$, given a correct candidate pair.  Technically, this information is available in the present case since the brute-force method was carried out above.  However, this is not a typical scenario, so we will use this as a comparative method only.  If no information is available then we will need to come up with a "best guess" as to the relationship.  With this in mind, we assume that the likelihood drops off exponentially as a function of the similarity score, with the probability at $sim=0.1$ exactly half that of $sim=0.0$, and $sim=0.2$ half that of $sim=0.1$, and so on.  In other words, we assume the relation, $P = e^{-\lambda x}$, where $\lambda = \frac{ln(2)}{t_{1/2}}$ and $t_{1/2} = 0.1$,
 
 
```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width="50%", fig.align="center"}
xs <- seq(0, 0.5, by=0.01)
fctn <- function(x,T=0.1) {exp(-(log(2)/T)*x)}
data.frame(x=xs, y=fctn(xs)) %>%
  ggplot() +
  geom_line(aes(x=x,y=y)) +
  labs(x="Jaccard Similarity", y="Likelihood of a correct match") +
  theme_minimal()

```

The value for $Pr(\theta)$ is the prior probability that the candidate pair is correct.  This depends on the number of candidate pairs, plus the possibility that _none_ of the candidate pairs are correct.  Assuming a uniform distribution, we use $\frac{1}{n+1}$, where $n$ is the number of candidate pairs.

The normalization factor can be expanded to, 

$$
Pr(y) = \sum_{\theta}{Pr(\theta) Pr(y | \theta)}
$$
This will be evaluated for each candidate pair determined for a given trout water, plus the possibility that none of the candidates are correct.  Since there is no information as to the probability that none of the candidate pairs is correct, we are forced to come up with a "best guess".  Considering that the VDEQ dataset is supposed to be comprehensive, the likelihood that a given trout waterway is not present is likely very small.  However, there is also the possibility that the waterway is present, but the name is sufficiently different that it is not offered as a candidate pair.  Given these factors, we estimate that there is a 2% likelihood that the waterway is not represented as a candidate pair.

<details>
<summary style="font-size:15px">CODE</summary>
```{r, message=FALSE, warning=FALSE}
faker_frame <- function(n) {
  data.frame(WATER_NAME = "None",
             P_cn = 0.02,
             P_pr = 1/(n+1))
}

bsn <- function(df) {
  df %>%
    mutate(P_cn = fctn(jc),
           P_pr = 1/(nrow(df)+1)) %>%
    bind_rows(faker_frame(nrow(df))) %>%
    mutate(P_yy = sum(P_cn*P_pr),
           P = P_cn*P_pr/P_yy)
}

df.adj1 <- df.cnd1 %>% 
  tidyr::nest(data=c(ID305B, WATER_NAME, source,coord, jc)) %>%
  mutate(data = purrr::map(data,bsn)) %>%
  tidyr::unnest(cols=c(data))

df.adj2 <- df.cnd2 %>% 
  tidyr::nest(data=c(ID305B, WATER_NAME, source,coord, jc)) %>%
  mutate(data = purrr::map(data,bsn)) %>%
  tidyr::unnest(cols=c(data))
```
</details>
<br>

Combining all of this into Bayes Law gives normalized probability values for each candidate pair, plus an additional option that none are correct.  A sample of the results is listed in the table below.  Note that the probability sums to 1.0 for each trout water, although in some cases this appears not to be the case due to rounding carried out for display purposes. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
df.adj1 %>%
  dplyr::select(id, waterbody1, WATER_NAME, jc, P) %>%
  head(n=10) %>%
  kbl(col.names=c("id","Trout Water", "VDEQ water", "Jaccard", "Probability"), digits=2) %>%
  kable_styling(full_width=FALSE) %>%
  kable_paper()
```


A close inspection of the sample results listed in the table above doesn't reveal anything overly surprising.  For example, in the case of "Irish Creek" there were two exact string matches and each are assigned approximately a 50% chance of being correct, with a very small probability that none are correct. The results for "South Fork Holston River" are truncated in the table above, but there were 13 candidate pairs, 3 of which are exact string matches.  The model gives a 68% chance that one of these exact matches are correct, and a 32% chance that one of the ten other candidate pairs are correct (plus a tiny chance that none are correct).

<br>

#### Dealing with multiplicity

In cases where a single trout water yielded two or more string matches with exactly the same name, it is useful to take a step back and consider that this likely are, in fact, the same waterways.  Recall the example above where the James River extended most of the way across the state, but it was divided into 41 individual segments.  Given this, we can deal with the multiplicity problem by simply merging these data together.  One way to check if this is reasonable is to aggregate by waterway, connect the dots between the candidate pairs, and compare with the waterway lines included in the original Rivers/Streams dataset,

<details>
<summary style="font-size:15px">CODE</summary>
```{r, message=FALSE, warning=FALSE}
# filter to just max prob
maxonly <- function(df) {df %>% filter(P == max(P))}
make_line <- function(lonlat) { lonlat %>% as.matrix() %>% sf::st_linestring() }


df.mx1 <- df.adj1 %>%
  dplyr::select(id, waterbody1,ID305B, WATER_NAME, source,coord, jc,P) %>%
  tidyr::nest(data=c(ID305B, WATER_NAME, source,coord, jc,P)) %>%
  mutate(data = purrr::map(data, maxonly),
         len  = purrr::map_int(data,nrow)) %>%
  tidyr::unnest(cols=c(data)) 

df.mx2 <- df.mx1 %>%
  filter(len>1) %>%
  group_by(id) %>%
  summarise(m = sf::st_coordinates(coord)) %>% 
  arrange(id,m) %>%
  tidyr::nest() %>%              
  mutate(ln = purrr::map(data, make_line) %>% sf::st_sfc(crs=EPSG_WGS84),
         coord2 = ln %>% sf::st_transform(epsg_code) %>% sf::st_centroid() %>% sf::st_transform(EPSG_WGS84)) 



df.mx <- bind_rows(
  df.mx1 %>% 
    filter(len==1) %>%
    dplyr::select(-ID305B, -source),
  df.mx1 %>% 
    filter(len>1) %>% 
    group_by(id, waterbody1, WATER_NAME) %>% 
    summarise(P=sum(P), jc=mean(jc), len=mean(len)) %>%
    left_join(df.mx2 %>% dplyr::select(id, coord=coord2), by=c("id")))
```
</details>


```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width="60%", fig.align="center"}
df.rv <- df.rvr %>%
  mutate(geometry=geometry %>% sf::st_transform(4326)) %>%
  filter(ID305B %in% df.mx1$ID305B)

df.mx2 %>%
  ggplot() +
  geom_sf(data=vaco1, mapping=aes(geometry=geometry), size=0.2, fill="grey95", color="grey65") +
  geom_sf(data=df.rv, mapping=aes(geometry=geometry), color="#74a9cf", size=0.5) +
  geom_sf(aes(geometry=ln), size=0.2, color="grey20") +
  theme_void()
```

With a few exceptions, the derived lines (black) more or less trace along the VDEQ (blue) waterways (in the original dataset these are represented as lines).  This implies that we can, in fact, combine identically-named waterways from the data. Once we combine the identical waterways and sum the respective probabilities we're left with a comprehensive list,


```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
#
# Do the whole analysis with the VDEQ Trout data instead, for comparison
#

df.deqt <- readRDS("../data/rds/df_deq_t.rds")


#####
d <- lapply(1:nrow(df.trt), function(i) {
  df.deqt %>% 
    filter(county == df.trt$county[i]) %>%
    mutate(waterbody1 = df.trt$waterbody1[i],
           jc = stringdist::stringdist(waterbody1, WATER_NAME, method="jaccard",q=3),
           id = df.trt$id[i]) %>%
    filter(jc<0.50) %>%
    arrange(id, jc) %>%
    dplyr::select(id, waterbody1, WQS_ID, WATER_NAME, county, jc, src, coord)
})
df.cndt <- do.call(rbind, d)
#####



#####
df.adjt <- df.cndt %>% 
  tidyr::nest(data=c(WQS_ID, WATER_NAME, src,coord, jc)) %>%
  mutate(data = purrr::map(data,bsn)) %>%
  tidyr::unnest(cols=c(data))
#####



#####
df.mxt1 <- df.adjt %>%
  dplyr::select(id, waterbody1,WQS_ID, WATER_NAME, src,coord, jc,P) %>%
  tidyr::nest(data=c(WQS_ID, WATER_NAME, src,coord, jc,P)) %>%
  mutate(data = purrr::map(data, maxonly),
         len  = purrr::map_int(data,nrow)) %>%
  tidyr::unnest(cols=c(data)) 

df.mxt2 <- df.mxt1 %>%
  filter(len>1) %>%
  group_by(id) %>%
  summarise(m = sf::st_coordinates(coord)) %>% 
  arrange(id,m) %>%
  tidyr::nest() %>%              
  mutate(ln = purrr::map(data, make_line) %>% sf::st_sfc(crs=EPSG_WGS84),
         coord2 = ln %>% sf::st_transform(epsg_code) %>% sf::st_centroid() %>% sf::st_transform(EPSG_WGS84)) 



df.mxt <- bind_rows(
  df.mxt1 %>% 
    filter(len==1) %>%
    dplyr::select(-WQS_ID, -src),
  df.mxt1 %>% 
    filter(len>1) %>% 
    group_by(id, waterbody1, WATER_NAME) %>% 
    summarise(P=sum(P), jc=mean(jc), len=mean(len)) %>%
    left_join(df.mxt2 %>% dplyr::select(id, coord=coord2), by=c("id")))
#####
```





```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width="60%", fig.align="center"}
df.mx %>% 
  dplyr::select(id, waterbody1, WATER_NAME, P) %>%
  head(n=10) %>%
  kbl(col.names=c("id", "Trout Water", "VDEQ Water", "Probability"), digits=2) %>%
  kable_styling(full_width=FALSE) %>%
  kable_paper()
```

<br>


<b>TODO: Compare results with "known" locations.</b>


## References

<ul>
  <li>MinHash vignette for the `textreuse` package [here](https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html)</li>
  <li>The book chapter from [Mining Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf)</li>
  <li>MinHash for dummies [post](http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html)</li>
  <li>A [Medium article](https://towardsdatascience.com/locality-sensitive-hashing-how-to-find-similar-items-in-a-large-set-with-precision-d907c52b05fc) on locality-sensitive hashing</li>
  <li>An [illustrated guide](https://www.pinecone.io/learn/locality-sensitive-hashing/) to LSH</li>
  <li>Comments on using `tokenizers` output with `textreuse` [here](https://github.com/ropensci/textreuse/issues/75)</li>
</ul>

<br>
<hr>
<br>