---
title: "troutr"
author: "Nathan Hoteling"
date: "1/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
library(stringdist)
library(osmdata)
library(ggplot2)

library(textreuse)

source("R/utils.R")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
va <- sf::st_read("data/gis/cb_2019_us_state_20m/cb_2019_us_state_20m.shp", quiet=TRUE) %>% 
  filter(NAME=="Virginia") %>%
  sf::st_transform(4326)

vaco1 <- sf::st_read("data/gis/cb_2019_us_county_20m/cb_2019_us_county_20m.shp", quiet=TRUE) %>%
  filter(STATEFP == "51") %>%
  sf::st_transform(4326)

# combine City/County with same name, to avoid confusion
vaco2 <- vaco1 %>% group_by(NAME) %>% summarise(geometry = sf::st_union(geometry))

df.trout <- readRDS("data/rds/dftrout.rds")

df.trt <- df.trout %>% 
  dplyr::select(waterbody1, waterbody2, Category, specialtype, county) %>%
  distinct() %>%
  mutate(id = paste0("trt-",row_number()))
```


<hr>
<br>


# Appendix: Finding Trout-Water Locations

Water bodies that are included in the Virginia Department of Wildlife Recreation (VDWR) trout stocking program can be browsed on an [interactive map](https://dgif-virginia.maps.arcgis.com/apps/webappviewer/index.html?id=441ed456c8664166bb735b1db6024e48), however there doesn't appear to be a simple way for the average user to download the location data.  For this reason, we explore some methods for determining these locations in the sections below.

<br>

## Brute Force Method

The no-tech solution to this problem is to simply brute-force our way through the trout water locations by manually searching for each individual location.  It's frustrating to know that the locations are definitely recorded in a database somewhere, just not easily accessible to me personally, so I literally pasted each "trout water" into the search bar on the interactive map, navigated to the appropriate search result, dropped a pin on the location, and copied the relevant coordinates.  

As painful as this was, it was manageable (it took about 3-ish hours), and very illuminating.  For example, I found a handful of inconsistencies like misspellings, alternative names, or missing waterways.  In some cases I resorted to a manual google-maps search to help narrow the field and made a judgement call as to the probable location of some waterway.  In all cases I entered a discrete point, even though many of the locations are rivers and streams and trout stocking is probably carried out at a variety of locations along the waterway.  In one case I wasn't able to find the location at all because the name, "Forest Service Office Youth Ponds" was very much ambiguous and I couldn't find where the Forest Service Office is located (apparently even Google doesn't know!); in that case I used the center of the respective county. 

<br>

## VDEQ Data

```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# First, download these files from:
# https://geohub-vadeq.hub.arcgis.com/pages/Water%20Datasets
# Then read them with sf
df.rvr <- sf::st_read("data/gis/Rivers_(2020_Final_WQA_IR_Assessment)/Rivers_(2020_Final_WQA_IR_Assessment).shp", quiet=TRUE)
df.res <- sf::st_read("data/gis/Reservoirs_(2020_Final_WQA_IR_Assessment)/Reservoirs_(2020_Final_WQA_IR_Assessment).shp", quiet=TRUE)


epsg_code <- spaceheater::get_epsg(df.rvr$geometry)
EPSG_WGS84 <- spaceheater::epsg_wgs84()

find_county <- function(crd) {
  vv <- sf::st_intersects(crd, vaco1$geometry) %>% unlist()
  nm <- ifelse(length(vv) > 0, vaco1$NAME[vv], NA)
  return(nm)
}


# Combine Rivers & Streams with Lakes & Reservoirs
df.deq <- bind_rows(
  # Rivers & Streams
  df.rvr %>%
  mutate(coord = geometry %>% sf::st_transform(epsg_code) %>% sf::st_centroid() %>% sf::st_transform(EPSG_WGS84),
         source = "rvr") %>%
  as_tibble() %>%
  dplyr::select(ID305B, WATER_NAME, LOCATION, source, coord),
  # Lakes & Reservoirs
  df.res %>% 
      mutate(coord = geometry %>% sf::st_transform(epsg_code) %>% sf::st_centroid() %>% sf::st_transform(EPSG_WGS84),
             source = "res") %>%
      as_tibble() %>%
      dplyr::select(ID305B, WATER_NAME, LOCATION, source, coord)
) %>%
  mutate(county = purrr::map_chr(coord, find_county))


# Note: some of the coordinates are outside of the state because
# of how the crds are derived above.  Note also that this could, in 
# some rare cases, mean that the county is also incorrect?
df.sub <- df.deq %>% 
  filter(is.na(county)) %>%
  mutate(coord2 = coord %>% 
           sf::st_transform(epsg_code) %>% 
           sf::st_buffer(dist=5000) %>% 
           sf::st_transform(EPSG_WGS84),
         county2 = purrr::map_chr(coord2,find_county))


df.deqx <- df.deq %>% 
  left_join(df.sub %>% dplyr::select(ID305B, county2), by=c("ID305B")) %>%
  mutate(county = ifelse(is.na(county), county2, county)) %>%
  dplyr::select(-county2)


# Save files
saveRDS(df.deqx, "data/rds/df_deq.rds")
readr::write_csv(df.deqx, "data/csv/df_deq.csv")
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
df.rvr <- sf::st_read("data/gis/Rivers_(2020_Final_WQA_IR_Assessment)/Rivers_(2020_Final_WQA_IR_Assessment).shp", quiet=TRUE)
df.res <- sf::st_read("data/gis/Reservoirs_(2020_Final_WQA_IR_Assessment)/Reservoirs_(2020_Final_WQA_IR_Assessment).shp", quiet=TRUE)

df.deq <- readRDS("data/rds/df_deq.rds")
```



The Virginia Department of Environmental Quality (VDEQ) maintains an up-to-date [collection of datasets](https://geohub-vadeq.hub.arcgis.com/pages/Water%20Datasets) related to rivers/streams, and lakes/reservoirs, plus a variety of other features.  Combining the rivers/streams and lakes/reservoirs data yields `r unique(df.deq$ID305B) %>% length() %>% format(big.mark=",")` unique bodies of water, or `r unique(df.deq$WATER_NAME) %>% length() %>% format(big.mark=",")` uniquely named bodies of water.  What's the difference?  Many of the bodies of water are divided into multiple segments and some are frequently-used names.  For example, the James River has `r df.deq %>% filter(WATER_NAME=="James River") %>% nrow()` sections outlined in the VDEQ data, and Mill Creek has `r df.deq %>% filter(WATER_NAME=="James River") %>% nrow()`.  If we look at the geographic distribution of the respective points, the difference becomes clear,

```{r, echo=FALSE, out.width="50%"}
df.deq %>% 
  filter(WATER_NAME %in% c("James River", "Mill Creek")) %>% 
  ggplot() + 
  geom_sf(data=vaco1, mapping=aes(geometry=geometry), size=0.2, fill="grey95", color="grey65") +
  geom_sf(aes(geometry=coord, color=WATER_NAME)) + 
  scale_color_discrete(name="Water Name") +
  theme_void() +
  theme(legend.position = c(0.2, 0.8))


```

To a large degree these multiplicity issues can be resolved by filtering the data by county (whereas counties aren't explicitly listed in the VDEQ data, we derive these from shapes available from the [Census Bureau](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html)).  Rivers, however, may require an extra step towards disambiguation.  Nevertheless, it's worth noting that around 75% of the names have only a single entry, so these problems shouldn't be too pervasive.


### Pairwise comparisons

Comparing water bodies between these two datasets is nontrivial.  As noted in the section above describing the brute-force approach, there are several cases where misspellings, alternative spellings, ambiguous names, or missing data come into play.  Add to this the multiplicities highlighted above for the VDEQ data.  With all of this in mind, the strategy here will be to formulate a set of logical rules that will help us come up with a "best guess" as to which trout waters denoted in the VDWR data correspond to which water bodies in the VDEQ data.  The initial procedure will look like this,

<ol>
  <li>Filter by county</li>
  <li>Determine name similarity</li>
  <li>Filter by jaccard distance</li>
</ol>


```{r, message=FALSE, warning=FALSE}

# Pairwise comparisons
t0 <- Sys.time()
d <- lapply(1:nrow(df.trt), function(i) {
  df.deq %>% 
    filter(county == df.trt$county[i]) %>%
    mutate(waterbody1 = df.trt$waterbody1[i],
           #jw = stringdist::stringdist(waterbody1, WATER_NAME, method="jw"),
           jc = stringdist::stringdist(waterbody1, WATER_NAME, method="jaccard",q=3),
           id = df.trt$id[i]) %>%
    filter(jc<0.50) %>%
    arrange(jc) %>%
    dplyr::select(id, waterbody1, ID305B, WATER_NAME, county, jc, source, coord)
})

df.cnd1 <- do.call(rbind, d)
t1 <- Sys.time()

DT_PR <- t1-t0
```



### Locality-sensitive hashing

Locality-sensitive hashing is an approximative technique that can be used to reduce the search space in cases where pairwise comparisons between datasets may become intractable.  The datasets we are working with don't quite meet that threshold, however it is interesting to go through the mechanics of LSH anyway.

The overarching goal here is to use the LSH techniques to create a list of candidate pairs.  Most of the necessary steps are carried out with the `textreuse` package (however we will do some quick pre-processing to tokenize the water body names via `tokenizers` since the built-in functionality tokenizes by _word_ whereas we want to tokenize by _character_),

```{r}
# Pre-processing: VDEQ data
df.deq2 <- df.deq %>%
  mutate(ngrams     = tokenizers::tokenize_character_shingles(WATER_NAME,        # tokenize by ngram
                                                              n=3,
                                                              strip_non_alphanum=FALSE,simplify=TRUE),  
         len        = purrr::map_int(ngrams, length),                            # number of tokens
         txt_tokens = purrr::map_chr(ngrams, str_c,collapse=" "))                # prep for minhash


# Pre-processing: VDWR data
df.trt2 <- df.trt %>%
  mutate(ngrams = tokenizers::tokenize_character_shingles(waterbody1, n=3,
                                                          strip_non_alphanum=FALSE,
                                                          simplify=TRUE),
         len = purrr::map_int(ngrams, length),
         txt_tokens = purrr::map_chr(ngrams, str_c, collapse=" "))
```

We have tokenized the water body names into ngrams, where n=3.  It is useful to take a look at the number of ngrams per name for each dataset,

```{r, out.width="50%", echo=FALSE, message=FALSE, warning=FALSE, fig.pos="hold"}
df.trt2 %>% 
  ggplot() +
  geom_histogram(aes(x=len), bins=25) +
  lims(x=c(0,50)) +
  labs(title="VDWR Trout Waters", x = "Number of ngrams per row", y="Count") +
  theme_minimal()

df.deq2 %>% 
  ggplot() +
  geom_histogram(aes(x=len), bins=25) +
  lims(x=c(0,50)) +
  labs(title="VDEQ Water Bodies", x = "Number of ngrams per row", y="Count") +
  theme_minimal()
```

The minhashing workflow here is based largely on the `textreuse` package [vignette](https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html).  We start by generating a minhash function and using that to create a document corpus containing all of the minhashed text strings.  The minhash function converts a collection of tokens (ngrams in our case) into `n` randomly selected hashes.  In the present dataset, there are `r df.trt2 %>% pull(ngrams) %>% unlist() %>% unique() %>% length() %>% format(big.mark=",")` unique ngrams derived from the trout waters, and `r df.deq2 %>% pull(ngrams) %>% unlist() %>% unique() %>% length() %>% format(big.mark=",")` from the VDEQ water body names.  These are mapped to 240 hashes,

```{r}
# Create minhash function
minhash <- textreuse::minhash_generator(n=240, seed=1234)
```

We create the document corpus consisting of all names from both datasets,

```{r}
# Create a named vector of tokens for each row
vec_tokens <- c(df.deq2$txt_tokens, df.trt2$txt_tokens)
names(vec_tokens) <- c(df.deq2$ID305B, df.trt2$id)

# Generate corpus
t0 <- Sys.time()
corpus <- textreuse::TextReuseCorpus(text = vec_tokens, tokenizer = tokenizers::tokenize_ngrams, n = 3,
                          minhash_func = minhash, keep_tokens = TRUE,
                          progress = FALSE, simplify=TRUE)
t1 <- Sys.time()
DT_LS1 <- t1-t0
```

Locality-sensitive hashing can now be carried out on the corpus.  The only remaining parameter is the number of bands to use in the computation.  This value approximates the threshold value for a jaccard similarity used to create the candidate pairs.  In the present case we will use $b=40$ since it leads to an approximate threshold of around 0.5.  


```{r, echo=FALSE, out.width="25%", eval=FALSE}
v <- c(10, 15, 30, 40, 60, 80, 120, 240)
d <- lapply(1:length(v), function(i) {
  lsh_threshold(h=240, b=v[i])
})
data.frame(b = v, similarity = unlist(d) %>% round(digits=3)) %>%
  kableExtra::kbl() %>%
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(full_width=FALSE)
```


```{r}
t0 <- Sys.time()
buckets_040    <- lsh(corpus, bands = 40, progress = FALSE)
candidates_040 <- lsh_candidates(buckets_040) %>% mutate(bands=40)
t1 <- Sys.time()
DT_LS2 <- t1-t0
```

The result is a list of candidate pairs, nominally, pairs which will yield a jaccard similarity less than the theshold value implied by our choice of $b$.  With a little bit of post-processing we can generate a result analogous to what was produced above,

```{r}
t0 <- Sys.time()
df.cnd2 <- candidates_040 %>%
  filter(a %in% df.trt2$id) %>%
  rename(id=a, ID305B=b) %>%
  left_join(df.trt %>% dplyr::select(id, waterbody1, county), by=c("id")) %>%
  left_join(df.deq %>% dplyr::select(ID305B, WATER_NAME, source, coord, county2=county), by=c("ID305B")) %>%
  filter(county==county2) %>%
  mutate(jc = stringdist::stringdist(waterbody1, WATER_NAME, method="jaccard",q=3)) %>%
  arrange(id, jc) %>%
  dplyr::select(id, waterbody1, ID305B, WATER_NAME, county, jc, source, coord)
t1 <- Sys.time()
DT_LS3 <- t1-t0
DT_LS <- DT_LS1 + DT_LS2 + DT_LS3
```

```{r, echo=FALSE, eval=FALSE}
# One can also use textreuse to compute the similarity.
# I used the above method so I can compare with the pairwise method
df.compare2 <- candidates_040 %>% filter(a %in% df.trt2$id) %>%
  lsh_compare(corpus, jaccard_similarity, progress=FALSE) %>%
  arrange(a,desc(score))
```

### Comparison of Pairwise and LSH methods

Now that we've done essentially the same thing using direct pairwise comparisons and Locality-sensitive hashing, it is possible to compare the results of each.  

First, ignoring some of the pre-processing steps, the Pairwise method took about <b>`r DT_PR %>% as.numeric() %>% format(digits=0)` s</b> to run, whereas the LSH method took <b>`r DT_LS %>% as.numeric() %>% format(digits=0)` s</b> to generate the corpus (`r DT_LS1 %>% as.numeric() %>% format(digits=1)`s), compute locality-sensitive hashes (`r DT_LS2 %>% as.numeric() %>% format(digits=1)`s), and carry out some light post-processing (`r DT_LS3 %>% as.numeric() %>% format(digits=1)`s).  

It's not a completely fair comparison, and there may be more optimization to be carried out here.  For example, the pairwise method doesn't compare all records with all other records; instead it compares each record from the trout data to all records in the VDEQ data that are in the same county.  I haven't figured out how to add this blocking step ahead of the LSH workflow, so I do this as part of the post-processing step.  Not only that, but the corpus doesn't distinguish between datasets, so the LSH method is intrinsically comparing each record with all other records in both datasets.  There may be a better way to structure this, as the documentation for the `lsh()` function includes the statement, "it is possible, as long as one uses the same minhash function and the same number of bands, to combine the outputs from this function at different times".  With this in mind, I wonder if we can run the function on a subsets of the corpus??


### Adjudication

TODO

<br>

## Other Approaches

OSM

```{r, echo=FALSE, eval=FALSE}
# Try osmdata for geocoding
#d <- lapply(1:nrow(df.mtch), function(i) {
#  s <- paste(df.mtch$nm[i], ", VA",sep="")
#  q <- osmdata::getbb(s) %>% bbox_to_pgon()
#  df <- data.frame(nm=df.mtch$nm[i], osm=q %>% sf::st_sfc() %>% sf::st_set_crs(4326))
#})
#df.mtch3 <- df.mtch2 %>% left_join(do.call(rbind,d), by="nm")
```



## References

<ul>
  <li>MinHash vignette for the `textreuse` package [here](https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html)</li>
  <li>MinHash for dummies [post](http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html)</li>
  <li>A [Medium article](https://towardsdatascience.com/locality-sensitive-hashing-how-to-find-similar-items-in-a-large-set-with-precision-d907c52b05fc) on locality-sensitive hashing</li>
  <li>Comments on using `tokenizers` output with `textreuse` [here](https://github.com/ropensci/textreuse/issues/75)</li>
</ul>
